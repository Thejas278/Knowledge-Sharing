Kafka
  It is a distributed Streaming platform
  Real Time Event Driven Application
  Fault Tolerant
  Scalable
  Kafka Runs in a cluster
  
  RabitMq vs Kafka
    With Kafka message when sent to the topic dont dissappear, but in case of rabitMq message dissappear as they are consumed.
    In case of kafka you can have bunch of consumers to read the data from since it does not dissappear
    
    Kafka Connect Source and Kafka Connect Sinks
      Source is used to move the data from datasource to Kafka
      Sink is used to move the data from Kafka to datasource both of these are java applications
      
      Kafka Streams Java API
        These are use to data transformation , Filtering,Joining etc..
    

Topic- It is used to publish or consume something from kafka.

  Topic is a log of events.
  we can make message in toipcs expire based on the size of the topic or by age
  Events in kafka are immutable
  
  We need to add the KafkaAdmin Spring bean, which will automatically add topics for all beans of type NewTopic:
  
              @Configuration
          public class KafkaTopicConfig {

              @Value(value = "${spring.kafka.bootstrap-servers}")
              private String bootstrapAddress;

              @Bean
              public KafkaAdmin kafkaAdmin() {
                  Map<String, Object> configs = new HashMap<>();
                  configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress);
                  return new KafkaAdmin(configs);
              }

              @Bean
              public NewTopic topic1() {
                   return new NewTopic("baeldung", 1, (short) 1);
              }
          }
  
Producing Messages
  To create messages, we first need to configure a ProducerFactory. This sets the strategy for creating Kafka Producer instances.

  Then we need a KafkaTemplate, which wraps a Producer instance and provides convenience methods for sending messages to Kafka topics.

  Producer instances are thread safe. So, using a single instance throughout an application context will give higher performance.
  Consequently, KakfaTemplate instances are also thread safe, and use of one instance is recommended.
  
                            @Configuration
                      public class KafkaProducerConfig {

                          @Bean
                          public ProducerFactory<String, String> producerFactory() {
                              Map<String, Object> configProps = new HashMap<>();
                              configProps.put(
                                ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, 
                                bootstrapAddress);
                              configProps.put(
                                ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                                StringSerializer.class);
                              configProps.put(
                                ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                                StringSerializer.class);
                              return new DefaultKafkaProducerFactory<>(configProps);
                          }

                          @Bean
                          public KafkaTemplate<String, String> kafkaTemplate() {
                              return new KafkaTemplate<>(producerFactory());
                          }
                      }
  Consuming Messages
            For consuming messages, we need to configure a ConsumerFactory and a KafkaListenerContainerFactory. Once these beans are available in the Spring bean factory, POJO-based consumers can be configured using @KafkaListener annotation.

@EnableKafka annotation is required on the configuration class to enable the detection of @KafkaListener annotation on spring-managed beans:

                                  @EnableKafka
                                  @Configuration
                                  public class KafkaConsumerConfig {

                                      @Bean
                                      public ConsumerFactory<String, String> consumerFactory() {
                                          Map<String, Object> props = new HashMap<>();
                                          props.put(
                                            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, 
                                            bootstrapAddress);
                                          props.put(
                                            ConsumerConfig.GROUP_ID_CONFIG, 
                                            groupId);
                                          props.put(
                                            ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                                            StringDeserializer.class);
                                          props.put(
                                            ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                                            StringDeserializer.class);
                                          return new DefaultKafkaConsumerFactory<>(props);
                                      }

                                      @Bean
                                      public ConcurrentKafkaListenerContainerFactory<String, String> 
                                        kafkaListenerContainerFactory() {

                                          ConcurrentKafkaListenerContainerFactory<String, String> factory =
                                            new ConcurrentKafkaListenerContainerFactory<>();
                                          factory.setConsumerFactory(consumerFactory());
                                          return factory;
                                      }
                                  }

                                  @KafkaListener(topics = "topicName", groupId = "foo")
                                  public void listenGroupFoo(String message) {
                                      System.out.println("Received Message in group foo: " + message);
                                  }
                                  
We can implement multiple listeners for a topic, each with a different group Id. Furthermore, one consumer can listen for messages from various topics:
          @KafkaListener(topics = "topic1, topic2", groupId = "foo")

For a topic with multiple partitions, however, a @KafkaListener can explicitly subscribe to a particular partition of a topic with an initial offset:

                  @KafkaListener(
          topicPartitions = @TopicPartition(topic = "topicName",
          partitionOffsets = {
            @PartitionOffset(partition = "0", initialOffset = "0"), 
            @PartitionOffset(partition = "3", initialOffset = "0")}),
          containerFactory = "partitionsKafkaListenerContainerFactory")
        public void listenToPartition(
          @Payload String message, 
          @Header(KafkaHeaders.RECEIVED_PARTITION_ID) int partition) {
              System.out.println(
                "Received Message: " + message"
                + "from partition: " + partition);
        }
        
        Since the initialOffset has been set to 0 in this listener, all the previously consumed messages from partitions 0 and 3 will be re-consumed every time
        this listener is initialized.
          
          Continue with this https://www.baeldung.com/spring-kafka
 Partitions Topics are broken down into smaller components called Partitions.
 
    when we wrire the message the message is actually stored in one of the topics partitions.
    Messages having the same key will end up in the same partition therefore mainitianing the order.
    If the message does not have key or null key then it is assigned to a partition in the round robin fashion ,but we will not maintian the order
    when the messages with keys are produced to Kafka and hash is computed which determines the partition
    
   Brokers- Brokers are the machines where the kafka process runs
        These are the network of machines
        we need to copy partition data to several brokers to keep the data safe, those copies are called Follower replica and the original one is called Leader replica
        Read and write happens to the leader
