Kafka
https://www.youtube.com/watch?v=vHbvbwSEYGo
https://www.youtube.com/watch?v=j4bqyAMMb7o&list=PLa7VYi0yPIH0KbnJQcMv5N9iW8HkZHztH
  It is a distributed Streaming platform
  Real Time Event Driven Application
  Fault Tolerant
  Scalable
  Kafka Runs in a cluster

Kafka is a event streaming platform used to collect ,store and process real time data streams at scale.

Kafka is used as event store

kafka holds data in immutable,append only (you can only append in the end) structure called topics

Topics are logs of events.

Kafka topics are durable because it is written to file system

Kafka is fast because it relies on Disk Caching and memory mapped files instead of jvm memory 
	Disk Caching means it works directly on the memory of the operating system most of the time

Topics are divided into units called partition

Now which message goes to which partition is based on the key which we pass . If we do not specify the key then the message are stored in a round robin fashion.
	If we pass the key then we find the hash of the key and mod it will the number of partions and resulting number is just the partition number to write. This will make sure that messages with the same key always goes to the same partition.

If you want ordered data then we must put data in single partition, Ordering is guarented inside partition

kafka is a distributes system because partition will divided the kafka topic so that it can be placed in different nodes of different kafka cluster, this is why kafka is scalable.

Multiple consumers cannot read from the same partitions
A consumer can read from multiple partitions

Producer configurations
key value serilizers
ack

	ack=all means producer will wait for all replias to return result as the default value for acknowledge property 
	ack=1 means only the broker that gets the request will acknowledge it will not wait for the replicas, doing this will increase the performance but will loose the resilliance
	ack=0 means no acknowledgement

compression.type
	
	this is set in producer , The compression is done by batch and improves with larger batch sizes
	Compression happens in producer and can be reused by broker and consumer

retires and timeouts can also be configured in producer

Kafka Brokers

Brokers can be a computer,or a instance running in the cloud or a container which runs the kafa process.
	brokers manage the partitions
	They manage replications of partitions

Usually its better to have the replication factor=number of brokers

Kafka producers are thread safe for multithreading

kafka admin dependency is required to create kafka topics programatically or we can use Confluent 

Kafka Raft Metadata mode
	
	Kafka uses the Raft consensus algorithm for leader election in its Kafka Raft Metadata mode, which eliminates the dependency on ZooKeeper for managing cluster metadata. The Raft algorithm is a consensus protocol designed to ensure fault-tolerant replication of state machines in a distributed system
	
	Read more about in https://medium.com/javarevisited/running-kafka-without-zookeeper-66b685db4991#:~:text=Kafka%20uses%20the%20Raft%20consensus%20algorithm%20for%20leader%20election%20in,machines%20in%20a%20distributed%20system.


Schema-registry 
	
	It will force the producers and consumers to use the schema which is registered.
	It supports backward compaibilty , it can evolve without any changes breaking



  
  RabitMq vs Kafka
    With Kafka message when sent to the topic dont dissappear, but in case of rabitMq message dissappear as they are consumed.
    In case of kafka you can have bunch of consumers to read the data from since it does not dissappear
    
    Kafka Connect Source and Kafka Connect Sink
      Source is used to move the data from datasource to Kafka
      Sink is used to move the data from Kafka to datasource both of these are java applications
      
      Kafka Streams Java API
        These are use to data transformation , Filtering,Joining etc..
    

Topic- It is used to publish or consume something from kafka.

  Topic is a log of events.
  we can make message in toipcs expire based on the size of the topic or by age
  Events in kafka are immutable
  
  We need to add the KafkaAdmin Spring bean, which will automatically add topics for all beans of type NewTopic:
  
              @Configuration
          public class KafkaTopicConfig {

              @Value(value = "${spring.kafka.bootstrap-servers}")
              private String bootstrapAddress;

              @Bean
              public KafkaAdmin kafkaAdmin() {
                  Map<String, Object> configs = new HashMap<>();
                  configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress);
                  return new KafkaAdmin(configs);
              }

              @Bean
              public NewTopic topic1() {
                   return new NewTopic("baeldung", 1, (short) 1);
              }
          }
  
Producing Messages
  To create messages, we first need to configure a ProducerFactory. This sets the strategy for creating Kafka Producer instances.

  Then we need a KafkaTemplate, which wraps a Producer instance and provides convenience methods for sending messages to Kafka topics.

  Producer instances are thread safe. So, using a single instance throughout an application context will give higher performance.
  Consequently, KakfaTemplate instances are also thread safe, and use of one instance is recommended.
  
                            @Configuration
                      public class KafkaProducerConfig {

                          @Bean
                          public ProducerFactory<String, String> producerFactory() {
                              Map<String, Object> configProps = new HashMap<>();
                              configProps.put(
                                ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, 
                                bootstrapAddress);
                              configProps.put(
                                ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                                StringSerializer.class);
                              configProps.put(
                                ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                                StringSerializer.class);
                              return new DefaultKafkaProducerFactory<>(configProps);
                          }

                          @Bean
                          public KafkaTemplate<String, String> kafkaTemplate() {
                              return new KafkaTemplate<>(producerFactory());
                          }
                      }
  Consuming Messages
            For consuming messages, we need to configure a ConsumerFactory and a KafkaListenerContainerFactory. Once these beans are available in the Spring bean factory, POJO-based consumers can be configured using @KafkaListener annotation.

@EnableKafka annotation is required on the configuration class to enable the detection of @KafkaListener annotation on spring-managed beans:

                                  @EnableKafka
                                  @Configuration
                                  public class KafkaConsumerConfig {

                                      @Bean
                                      public ConsumerFactory<String, String> consumerFactory() {
                                          Map<String, Object> props = new HashMap<>();
                                          props.put(
                                            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, 
                                            bootstrapAddress);
                                          props.put(
                                            ConsumerConfig.GROUP_ID_CONFIG, 
                                            groupId);
                                          props.put(
                                            ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                                            StringDeserializer.class);
                                          props.put(
                                            ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                                            StringDeserializer.class);
                                          return new DefaultKafkaConsumerFactory<>(props);
                                      }

                                      @Bean
                                      public ConcurrentKafkaListenerContainerFactory<String, String> 
                                        kafkaListenerContainerFactory() {

                                          ConcurrentKafkaListenerContainerFactory<String, String> factory =
                                            new ConcurrentKafkaListenerContainerFactory<>();
                                          factory.setConsumerFactory(consumerFactory());
                                          return factory;
                                      }
                                  }

                                  @KafkaListener(topics = "topicName", groupId = "foo")
                                  public void listenGroupFoo(String message) {
                                      System.out.println("Received Message in group foo: " + message);
                                  }
                                  
We can implement multiple listeners for a topic, each with a different group Id. Furthermore, one consumer can listen for messages from various topics:
          @KafkaListener(topics = "topic1, topic2", groupId = "foo")

For a topic with multiple partitions, however, a @KafkaListener can explicitly subscribe to a particular partition of a topic with an initial offset:

                  @KafkaListener(
          topicPartitions = @TopicPartition(topic = "topicName",
          partitionOffsets = {
            @PartitionOffset(partition = "0", initialOffset = "0"), 
            @PartitionOffset(partition = "3", initialOffset = "0")}),
          containerFactory = "partitionsKafkaListenerContainerFactory")
        public void listenToPartition(
          @Payload String message, 
          @Header(KafkaHeaders.RECEIVED_PARTITION_ID) int partition) {
              System.out.println(
                "Received Message: " + message"
                + "from partition: " + partition);
        }
        
        Since the initialOffset has been set to 0 in this listener, all the previously consumed messages from partitions 0 and 3 will be re-consumed every time
        this listener is initialized.
          
          Continue with this https://www.baeldung.com/spring-kafka
 Partitions Topics are broken down into smaller components called Partitions.
 
    when we wrire the message the message is actually stored in one of the topics partitions.
    Messages having the same key will end up in the same partition therefore mainitianing the order.
    If the message does not have key or null key then it is assigned to a partition in the round robin fashion ,but we will not maintian the order
    when the messages with keys are produced to Kafka and hash is computed which determines the partition
    
   Brokers- Brokers are the machines where the kafka process runs
        These are the network of machines
        we need to copy partition data to several brokers to keep the data safe, those copies are called Follower replica and the original one is called Leader replica
        Read and write happens to the leader
